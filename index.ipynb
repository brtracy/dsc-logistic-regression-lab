{"cells":[{"cell_type":"markdown","metadata":{"id":"SOu_X9eZ34yk"},"source":["# Logistic Regression - Cumulative Lab\n","\n","## Introduction\n","\n","In this cumulative lab, you will walk through a complete machine learning workflow with logistic regression, including data preparation, modeling (including hyperparameter tuning), and final model evaluation.\n","\n","## Objectives\n","\n","You will be able to:\n","\n","* Practice identifying and applying appropriate preprocessing steps\n","* Perform an iterative modeling process, starting from a baseline model\n","* Practice model validation\n","* Practice choosing a final logistic regression model and evaluating its performance"]},{"cell_type":"markdown","metadata":{"id":"H_fnfQfq34ym"},"source":["## Your Task: Complete an End-to-End ML Process with Logistic Regression on the Forest Cover Dataset\n","\n","![forest road](images/forest_road.jpg)\n","\n","<span>Photo by <a href=\"https://unsplash.com/@von_co?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Ivana Cajina</a> on <a href=\"https://unsplash.com/s/photos/forest-satellite?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span>"]},{"cell_type":"markdown","metadata":{"id":"gOP_xgmy34ym"},"source":["### Business and Data Understanding\n","\n","Here we will be using an adapted version of the forest cover dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/covertype). Each record represents a 30 x 30 meter cell of land within Roosevelt National Forest in northern Colorado, which has been labeled as `Cover_Type` 1 for \"Cottonwood/Willow\" and `Cover_Type` 0 for \"Ponderosa Pine\". (The original dataset contained 7 cover types but we have simplified it.)\n","\n","The task is to predict the `Cover_Type` based on the available cartographic variables:"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2022-01-11T21:13:25.714185Z","start_time":"2022-01-11T21:13:24.934237Z"},"colab":{"base_uri":"https://localhost:8080/","height":488},"id":"X1Dqy9YW34yn","executionInfo":{"status":"ok","timestamp":1642604805551,"user_tz":300,"elapsed":279,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"d7c35c89-4e0d-42a1-b553-7cd150071fa7"},"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-38f677ab-8552-4bfb-b5a6-01e05dc3027b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Elevation</th>\n","      <th>Aspect</th>\n","      <th>Slope</th>\n","      <th>Horizontal_Distance_To_Hydrology</th>\n","      <th>Vertical_Distance_To_Hydrology</th>\n","      <th>Horizontal_Distance_To_Roadways</th>\n","      <th>Hillshade_9am</th>\n","      <th>Hillshade_Noon</th>\n","      <th>Hillshade_3pm</th>\n","      <th>Horizontal_Distance_To_Fire_Points</th>\n","      <th>Wilderness_Area_1</th>\n","      <th>Wilderness_Area_2</th>\n","      <th>Wilderness_Area_3</th>\n","      <th>Soil_Type_1</th>\n","      <th>Soil_Type_2</th>\n","      <th>Soil_Type_3</th>\n","      <th>Soil_Type_4</th>\n","      <th>Soil_Type_5</th>\n","      <th>Soil_Type_6</th>\n","      <th>Soil_Type_7</th>\n","      <th>Soil_Type_8</th>\n","      <th>Soil_Type_9</th>\n","      <th>Soil_Type_10</th>\n","      <th>Soil_Type_11</th>\n","      <th>Soil_Type_12</th>\n","      <th>Soil_Type_13</th>\n","      <th>Soil_Type_14</th>\n","      <th>Soil_Type_15</th>\n","      <th>Soil_Type_16</th>\n","      <th>Soil_Type_17</th>\n","      <th>Soil_Type_18</th>\n","      <th>Soil_Type_19</th>\n","      <th>Soil_Type_20</th>\n","      <th>Soil_Type_21</th>\n","      <th>Soil_Type_22</th>\n","      <th>Soil_Type_23</th>\n","      <th>Soil_Type_24</th>\n","      <th>Soil_Type_25</th>\n","      <th>Soil_Type_26</th>\n","      <th>Soil_Type_27</th>\n","      <th>Soil_Type_28</th>\n","      <th>Soil_Type_29</th>\n","      <th>Soil_Type_30</th>\n","      <th>Soil_Type_31</th>\n","      <th>Soil_Type_32</th>\n","      <th>Soil_Type_33</th>\n","      <th>Soil_Type_34</th>\n","      <th>Soil_Type_35</th>\n","      <th>Soil_Type_36</th>\n","      <th>Soil_Type_37</th>\n","      <th>Soil_Type_38</th>\n","      <th>Soil_Type_39</th>\n","      <th>Cover_Type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2553</td>\n","      <td>235</td>\n","      <td>17</td>\n","      <td>351</td>\n","      <td>95</td>\n","      <td>780</td>\n","      <td>188</td>\n","      <td>253</td>\n","      <td>199</td>\n","      <td>1410</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2011</td>\n","      <td>344</td>\n","      <td>17</td>\n","      <td>313</td>\n","      <td>29</td>\n","      <td>404</td>\n","      <td>183</td>\n","      <td>211</td>\n","      <td>164</td>\n","      <td>300</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2022</td>\n","      <td>24</td>\n","      <td>13</td>\n","      <td>391</td>\n","      <td>42</td>\n","      <td>509</td>\n","      <td>212</td>\n","      <td>212</td>\n","      <td>134</td>\n","      <td>421</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2038</td>\n","      <td>50</td>\n","      <td>17</td>\n","      <td>408</td>\n","      <td>71</td>\n","      <td>474</td>\n","      <td>226</td>\n","      <td>200</td>\n","      <td>102</td>\n","      <td>283</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2018</td>\n","      <td>341</td>\n","      <td>27</td>\n","      <td>351</td>\n","      <td>34</td>\n","      <td>390</td>\n","      <td>152</td>\n","      <td>188</td>\n","      <td>168</td>\n","      <td>190</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>38496</th>\n","      <td>2396</td>\n","      <td>153</td>\n","      <td>20</td>\n","      <td>85</td>\n","      <td>17</td>\n","      <td>108</td>\n","      <td>240</td>\n","      <td>237</td>\n","      <td>118</td>\n","      <td>837</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>38497</th>\n","      <td>2391</td>\n","      <td>152</td>\n","      <td>19</td>\n","      <td>67</td>\n","      <td>12</td>\n","      <td>95</td>\n","      <td>240</td>\n","      <td>237</td>\n","      <td>119</td>\n","      <td>845</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>38498</th>\n","      <td>2386</td>\n","      <td>159</td>\n","      <td>17</td>\n","      <td>60</td>\n","      <td>7</td>\n","      <td>90</td>\n","      <td>236</td>\n","      <td>241</td>\n","      <td>130</td>\n","      <td>854</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>38499</th>\n","      <td>2384</td>\n","      <td>170</td>\n","      <td>15</td>\n","      <td>60</td>\n","      <td>5</td>\n","      <td>90</td>\n","      <td>230</td>\n","      <td>245</td>\n","      <td>143</td>\n","      <td>864</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>38500</th>\n","      <td>2383</td>\n","      <td>165</td>\n","      <td>13</td>\n","      <td>60</td>\n","      <td>4</td>\n","      <td>67</td>\n","      <td>231</td>\n","      <td>244</td>\n","      <td>141</td>\n","      <td>875</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>38501 rows × 53 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38f677ab-8552-4bfb-b5a6-01e05dc3027b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-38f677ab-8552-4bfb-b5a6-01e05dc3027b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-38f677ab-8552-4bfb-b5a6-01e05dc3027b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["       Elevation  Aspect  Slope  ...  Soil_Type_38  Soil_Type_39  Cover_Type\n","0           2553     235     17  ...             0             0           0\n","1           2011     344     17  ...             0             0           0\n","2           2022      24     13  ...             0             0           0\n","3           2038      50     17  ...             0             0           0\n","4           2018     341     27  ...             0             0           0\n","...          ...     ...    ...  ...           ...           ...         ...\n","38496       2396     153     20  ...             0             0           0\n","38497       2391     152     19  ...             0             0           0\n","38498       2386     159     17  ...             0             0           0\n","38499       2384     170     15  ...             0             0           0\n","38500       2383     165     13  ...             0             0           0\n","\n","[38501 rows x 53 columns]"]},"metadata":{},"execution_count":2}],"source":["# Run this cell without changes\n","import pandas as pd\n","\n","df = pd.read_csv('forest_cover.csv')  \n","df"]},{"cell_type":"markdown","metadata":{"id":"O0BL_B_934yn"},"source":["As you can see, we have over 38,000 rows, each with 52 feature columns and 1 target column:\n","\n","* `Elevation`: Elevation in meters\n","* `Aspect`: Aspect in degrees azimuth\n","* `Slope`: Slope in degrees\n","* `Horizontal_Distance_To_Hydrology`: Horizontal dist to nearest surface water features in meters\n","* `Vertical_Distance_To_Hydrology`: Vertical dist to nearest surface water features in meters\n","* `Horizontal_Distance_To_Roadways`: Horizontal dist to nearest roadway in meters\n","* `Hillshade_9am`: Hillshade index at 9am, summer solstice\n","* `Hillshade_Noon`: Hillshade index at noon, summer solstice\n","* `Hillshade_3pm`: Hillshade index at 3pm, summer solstice\n","* `Horizontal_Distance_To_Fire_Points`: Horizontal dist to nearest wildfire ignition points, meters\n","* `Wilderness_Area_x`: Wilderness area designation (3 columns)\n","* `Soil_Type_x`: Soil Type designation (39 columns)\n","* `Cover_Type`: 1 for cottonwood/willow, 0 for ponderosa pine"]},{"cell_type":"markdown","metadata":{"id":"k9dadJPL34yo"},"source":["This is also an imbalanced dataset, since cottonwood/willow trees are relatively rare in this forest:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYpWxoRm34yo","executionInfo":{"status":"ok","timestamp":1642604809743,"user_tz":300,"elapsed":152,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"c120140e-bea8-4d77-9c49-f3a91c80f2db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Raw Counts\n","0    35754\n","1     2747\n","Name: Cover_Type, dtype: int64\n","\n","Percentages\n","0    0.928651\n","1    0.071349\n","Name: Cover_Type, dtype: float64\n"]}],"source":["# Run this cell without changes\n","print(\"Raw Counts\")\n","print(df[\"Cover_Type\"].value_counts())\n","print()\n","print(\"Percentages\")\n","print(df[\"Cover_Type\"].value_counts(normalize=True))"]},{"cell_type":"markdown","metadata":{"id":"tSO8-d3-34yp"},"source":["If we had a model that *always* said that the cover type was ponderosa pine (class 0), what accuracy score would we get?"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"gOgFKylN34yp","executionInfo":{"status":"ok","timestamp":1642604813745,"user_tz":300,"elapsed":113,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"63b37b11-28bc-469d-f1d7-d836d3bd4869"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nWe would get about a 92% accuracy score\\n'"]},"metadata":{},"execution_count":4}],"source":["# Replace None with appropriate text\n","\"\"\"\n","We would get about a 92% accuracy score\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"GSF5-OHK34yq"},"source":["You will need to take this into account when working through this problem."]},{"cell_type":"markdown","metadata":{"id":"BOucyPB334yq"},"source":["### Requirements\n","\n","#### 1. Perform a Train-Test Split\n","\n","For a complete end-to-end ML process, we need to create a holdout set that we will use at the very end to evaluate our final model's performance.\n","\n","#### 2. Build and Evaluate a Baseline Model\n","\n","Without performing any preprocessing or hyperparameter tuning, build and evaluate a vanilla logistic regression model using log loss and `cross_val_score`.\n","\n","#### 3. Write a Custom Cross Validation Function\n","\n","Because we are using preprocessing techniques that differ for train and validation data, we will need a custom function rather than simply preprocessing the entire `X_train` and using `cross_val_score` from scikit-learn.\n","\n","#### 4. Build and Evaluate Additional Logistic Regression Models\n","\n","Using the function created in the previous step, build multiple logistic regression models with different hyperparameters in order to minimize log loss.\n","\n","#### 5. Choose and Evaluate a Final Model\n","\n","Preprocess the full training set and test set appropriately, then evaluate the final model with various classification metrics in addition to log loss."]},{"cell_type":"markdown","metadata":{"id":"FvHOV5SD34yq"},"source":["## 1. Perform a Train-Test Split\n","\n","This process should be fairly familiar by now. In the cell below, use the variable `df` (that has already been created) in order to create `X` and `y`, then training and test sets using `train_test_split` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n","\n","We'll use a random state of 42 and `stratify=y` (to ensure an even balance of tree types) in the train-test split. Recall that the target is `Cover_Type`."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Bxt4SSmb34yq","executionInfo":{"status":"ok","timestamp":1642604821743,"user_tz":300,"elapsed":939,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}}},"outputs":[],"source":["# Replace None with appropriate code\n","\n","# Import the relevant function\n","from sklearn.model_selection import train_test_split\n","\n","# Split df into X and y\n","X = df.drop(columns=['Cover_Type'], axis=1)\n","y = df['Cover_Type']\n","\n","# Perform train-test split with random_state=42 and stratify=y\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n","                                                   stratify=y)"]},{"cell_type":"markdown","metadata":{"id":"ExvUlacP34yr"},"source":["Check that you have the correct data shape before proceeding:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ZgXbPT0S34yr","executionInfo":{"status":"ok","timestamp":1642604824099,"user_tz":300,"elapsed":110,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}}},"outputs":[],"source":["# Run this cell without changes\n","\n","# X and y training data should have the same number of rows\n","assert X_train.shape[0] == y_train.shape[0] and X_train.shape[0] == 28875\n","\n","# X and y testing data should have the same number of rows\n","assert X_test.shape[0] == y_test.shape[0] and X_test.shape[0] == 9626\n","\n","# Both X should have 52 columns\n","assert X_train.shape[1] == X_test.shape[1] and X_train.shape[1] == 52\n","\n","# Both y should have 1 column\n","assert len(y_train.shape) == len(y_test.shape) and len(y_train.shape) == 1"]},{"cell_type":"markdown","metadata":{"id":"vxOwjmHg34yr"},"source":["Also, we should have roughly equal percentages of cottonwood/willow trees for train vs. test targets:"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJfMew2j34yr","executionInfo":{"status":"ok","timestamp":1642604826117,"user_tz":300,"elapsed":115,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"fd188bb4-cfd5-4724-9e1e-962b54811ea4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train percent cottonwood/willow: 0.07134199134199135\n","Test percent cottonwood/willow:  0.0713692083939331\n"]}],"source":["# Run this cell without changes\n","print(\"Train percent cottonwood/willow:\", y_train.value_counts(normalize=True)[1])\n","print(\"Test percent cottonwood/willow: \", y_test.value_counts(normalize=True)[1])"]},{"cell_type":"markdown","metadata":{"id":"xf4UbpqY34ys"},"source":["## 2. Build and Evaluate a Baseline Model\n","\n","Using scikit-learn's `LogisticRegression` model, instantiate a classifier with `random_state=42`. Then use `cross_val_score` with `scoring=\"neg_log_loss\"` to find the average cross-validated log loss for this model on `X_train` and `y_train`.\n","\n","* [`LogisticRegression` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n","* [`cross_val_score` documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n","\n","(Similar to RMSE, the internal implementation of `cross_val_score` requires that we use \"negative log loss\" instead of just log loss. The code provided negates the result for you.)\n","\n","**The code below should produce a warning** but not an error. Because we have not scaled the data, we expect to get a `ConvergenceWarning` five times (once for each fold of cross validation)."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XTCE6lFg34ys","executionInfo":{"status":"ok","timestamp":1642604837248,"user_tz":300,"elapsed":4724,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"c57adf12-7f3d-43ac-9406-3083a6e78782"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"output_type":"execute_result","data":{"text/plain":["0.17215925365425272"]},"metadata":{},"execution_count":8}],"source":["# Replace None with appropriate code\n","\n","# Import relevant class and function\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","\n","# Instantiate a LogisticRegression with random_state=42\n","baseline_model = LogisticRegression(random_state=42)\n","\n","# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n","# on X_train and y_train\n","baseline_neg_log_loss_cv = cross_val_score(baseline_model, X_train, y_train,\n","                                           scoring=\"neg_log_loss\")\n","\n","baseline_log_loss = -(baseline_neg_log_loss_cv.mean())\n","baseline_log_loss"]},{"cell_type":"markdown","metadata":{"id":"bdapdLUi34ys"},"source":["Ok, so we are getting the `ConvergenceWarning`s we expected, and log loss of around 0.172 with our baseline model.\n","\n","Is that a \"good\" log loss? That's hard to say — log loss is not particularly interpretable. \n","\n","If we had a model that just chose 0 (the majority class) every time, this is the log loss we would get:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6jHI5JlP34yt","executionInfo":{"status":"ok","timestamp":1642604846920,"user_tz":300,"elapsed":109,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"b0f42ef1-0b1c-4826-a73c-5018f8d814ec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.4640650865286937"]},"metadata":{},"execution_count":9}],"source":["# Run this cell without changes\n","from sklearn.metrics import log_loss\n","import numpy as np\n","\n","log_loss(y_train, np.zeros(len(y_train)))"]},{"cell_type":"markdown","metadata":{"id":"6hJeA8X534yt"},"source":["Loss is a metric where lower is better, so our baseline model is clearly an improvement over just guessing the majority class every time.\n","\n","Even though it is difficult to interpret, the 0.172 value will be a useful baseline as we continue modeling, to see if we are actually making improvements or just getting slightly better performance by chance.\n","\n","We will also use other metrics at the last step in order to describe the final model's performance in a more user-friendly way."]},{"cell_type":"markdown","metadata":{"id":"8mQ-pqJG34yt"},"source":["## 3. Write a Custom Cross Validation Function\n","\n","### Conceptual Considerations\n","\n","First, consider: which preprocessing steps should be taken with this dataset? Recall that our data is imbalanced, and that it caused a `ConvergenceWarning` for our baseline model."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"xZ8EsYcr34yt","executionInfo":{"status":"ok","timestamp":1642604861796,"user_tz":300,"elapsed":151,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"12d906d1-9a38-49d7-fca2-59b00ed8aebc"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nDue to the imbalance in the target, we should use SMOTE to account for it.\\n'"]},"metadata":{},"execution_count":10}],"source":["# Replace None with appropriate text\n","\"\"\"\n","Due to the imbalance in the target, we should use SMOTE to account for it.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"_kvxqjdp34yu"},"source":["As you likely noted above, we should use some kind of resampling technique to address the large class imbalance. Let's use `SMOTE` (synthetic minority oversampling, [documentation here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)), which creates synthetic examples of the minority class to help train the model.\n","\n","Does SMOTE work just like a typical scikit-learn transformer, where you fit the transformer on the training data then transform both the training and the test data?"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"JunkbZyv34yu","executionInfo":{"status":"ok","timestamp":1642604864619,"user_tz":300,"elapsed":122,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"41850588-b57b-42b2-c43e-cf7e164072b5"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nNo, SMOTE does not work like that. We never want to oversample the\\nminority class in the test data, because then we are generating\\nmetrics based on synthetic data and not actual data.\\n\\nInstead, we only want to fit and transform the training data, and\\nleave the testing data alone.\\n'"]},"metadata":{},"execution_count":11}],"source":["# __SOLUTION\n","\"\"\"\n","No, SMOTE does not work like that. We never want to oversample the\n","minority class in the test data, because then we are generating\n","metrics based on synthetic data and not actual data.\n","\n","Instead, we only want to fit and transform the training data, and\n","leave the testing data alone.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"wdqWX99N34yu"},"source":["As you also likely noted above, we should use some transformer to normalize the data. Let's use a `StandardScaler` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)).\n","\n","Does `StandardScaler` work just like a typical scikit-learn transformer, where you fit the transformer on the training data then transform both the training and the test data?"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"mHCaO-8D34yu","executionInfo":{"status":"ok","timestamp":1642604868457,"user_tz":300,"elapsed":152,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"1a420630-8042-458b-8482-9483e251cba3"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nYes\\n'"]},"metadata":{},"execution_count":12}],"source":["# Replace None with appropriate text\n","\"\"\"\n","Yes\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"C6Z0eEXO34yu"},"source":["(At this point it's a good idea to double-check your answers against the `solution` branch to make sure you understand the setup.)"]},{"cell_type":"markdown","metadata":{"id":"aPkDQGRf34yu"},"source":["### Using `StratifiedKFold`\n","\n","As you can see from the `cross_val_score` documentation linked above, \"under the hood\" it is using `StratifiedKFold` for classification tasks.\n","\n","Essentially `StratifiedKFold` is just providing the information you need to make 5 separate train-test splits inside of `X_train`. Then there is other logic within `cross_val_score` to fit and evaluate the provided model.\n","\n","So, if our original code looked like this:\n","\n","```python\n","baseline_model = LogisticRegression(random_state=42)\n","baseline_neg_log_loss_cv = cross_val_score(baseline_model, X_train, y_train, scoring=\"neg_log_loss\")\n","baseline_log_loss = -(baseline_neg_log_loss_cv.mean())\n","baseline_log_loss\n","```\n","\n","The equivalent of the above code using `StratifiedKFold` would look something like this:"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYFjkFco34yv","executionInfo":{"status":"ok","timestamp":1642604877503,"user_tz":300,"elapsed":4016,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"e71bcae0-af64-4cf1-9d22-392c38a91320"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"output_type":"execute_result","data":{"text/plain":["0.17215925365425272"]},"metadata":{},"execution_count":13}],"source":["# Run this cell without changes\n","from sklearn.metrics import make_scorer\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.base import clone\n","\n","# Negative log loss doesn't exist as something we can import,\n","# but we can create it\n","neg_log_loss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n","\n","# Instantiate the model (same as previous example)\n","baseline_model = LogisticRegression(random_state=42)\n","\n","# Create a list to hold the score from each fold\n","kfold_scores = np.ndarray(5)\n","\n","# Instantiate a splitter object and loop over its result\n","kfold = StratifiedKFold()\n","for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n","    # Extract train and validation subsets using the provided indices\n","    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n","    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n","    \n","    # Clone the provided model and fit it on the train subset\n","    temp_model = clone(baseline_model)\n","    temp_model.fit(X_t, y_t)\n","    \n","    # Evaluate the provided model on the validation subset\n","    neg_log_loss_score = neg_log_loss(temp_model, X_val, y_val)\n","    kfold_scores[fold] = neg_log_loss_score\n","    \n","-(kfold_scores.mean())"]},{"cell_type":"markdown","metadata":{"id":"Y2s8fE8o34yv"},"source":["As you can see, this produced the same result as our original cross validation (including the `ConvergenceWarning`s):"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oCdnhcp34yv","executionInfo":{"status":"ok","timestamp":1642604887418,"user_tz":300,"elapsed":110,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"9d0f8229-2bc5-4eb2-9f3e-ece997eb9299"},"outputs":[{"output_type":"stream","name":"stdout","text":["[-0.17160028 -0.17457711 -0.16303793 -0.1794859  -0.17209505]\n","[-0.17160028 -0.17457711 -0.16303793 -0.1794859  -0.17209505]\n"]}],"source":["# Run this cell without changes\n","print(baseline_neg_log_loss_cv)\n","print(kfold_scores)"]},{"cell_type":"markdown","metadata":{"id":"NMO9YDhl34yv"},"source":["So, what is the point of doing it this way, instead of the much-shorter `cross_val_score` approach?\n","\n","**Using `StratifiedKFold` \"by hand\" allows us to customize what happens inside of that loop.**\n","\n","Therefore we can apply these preprocessing techniques appropriately:\n","\n","1. Fit a `StandardScaler` object on the training subset (not the full training data) and transform both the train and test subsets\n","2. Fit a `SMOTE` object and transform only the training subset"]},{"cell_type":"markdown","metadata":{"id":"azbJdHrx34yw"},"source":["### Writing a Custom Cross Validation Function with `StratifiedKFold`\n","\n","In the cell below, we have set up a function `custom_cross_val_score` that has an interface that resembles the `cross_val_score` function from scikit-learn.\n","\n","Most of it is set up for you already, all you need to do is add the `SMOTE` and `StandardScaler` steps described above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSsXht1R34yw"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E44pcRJP34yw","executionInfo":{"status":"ok","timestamp":1642604906362,"user_tz":300,"elapsed":4817,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"3970e1e7-aa13-48d5-a44f-265cfc43ba6a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.13235899551356448"]},"metadata":{},"execution_count":17}],"source":["# Import relevant sklearn and imblearn classes\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","\n","def custom_cross_val_score(estimator, X, y):\n","    # Create a list to hold the scores from each fold\n","    kfold_train_scores = np.ndarray(5)\n","    kfold_val_scores = np.ndarray(5)\n","\n","    # Instantiate a splitter object and loop over its result\n","    kfold = StratifiedKFold(n_splits=5)\n","    for fold, (train_index, val_index) in enumerate(kfold.split(X, y)):\n","        # Extract train and validation subsets using the provided indices\n","        X_t, X_val = X.iloc[train_index], X.iloc[val_index]\n","        y_t, y_val = y.iloc[train_index], y.iloc[val_index]\n","        \n","        # Instantiate StandardScaler\n","        scaler = StandardScaler()\n","        # Fit and transform X_t\n","        X_t_scaled = scaler.fit_transform(X_t)\n","        # Transform X_val\n","        X_val_scaled = scaler.transform(X_val)\n","        \n","        # Instantiate SMOTE with random_state=42 and sampling_strategy=0.28\n","        sm = SMOTE(random_state=42, sampling_strategy=0.28)\n","        # Fit and transform X_t_scaled and y_t using sm\n","        X_t_oversampled, y_t_oversampled = sm.fit_resample(X_t_scaled, y_t)\n","        \n","        # Clone the provided model and fit it on the train subset\n","        temp_model = clone(estimator)\n","        temp_model.fit(X_t_oversampled, y_t_oversampled)\n","        \n","        # Evaluate the provided model on the train and validation subsets\n","        neg_log_loss_score_train = neg_log_loss(temp_model, X_t_oversampled, y_t_oversampled)\n","        neg_log_loss_score_val = neg_log_loss(temp_model, X_val_scaled, y_val)\n","        kfold_train_scores[fold] = neg_log_loss_score_train\n","        kfold_val_scores[fold] = neg_log_loss_score_val\n","        \n","    return kfold_train_scores, kfold_val_scores\n","\n","model_with_preprocessing = LogisticRegression(random_state=42, class_weight={1: 0.28})\n","preprocessed_train_scores, preprocessed_neg_log_loss_cv = custom_cross_val_score(model_with_preprocessing, X_train, y_train)\n","- (preprocessed_neg_log_loss_cv.mean())"]},{"cell_type":"markdown","metadata":{"id":"xsH2CHiY34yw"},"source":["The output you get should be about 0.132, and there should no longer be a `ConvergenceWarning`.\n","\n","If you're not getting the right output, double check that you are applying the correct transformations to the correct variables:\n","\n","1. `X_t` should be scaled to create `X_t_scaled`, then `X_t_scaled` should be resampled to create `X_t_oversampled`, then `X_t_oversampled` should be used to fit the model\n","2. `X_val` should be scaled to create `X_val_scaled`, then `X_val_scaled` should be used to evaluate `neg_log_loss`\n","3. `y_t` should be resampled to create `y_t_oversampled`, then `y_t_oversampled` should be used to fit the model\n","4. `y_val` should not be transformed in any way. It should just be used to evaluate `neg_log_loss`\n","\n","Another thing to check is that you used `sampling_strategy=0.28` when you instantiated the `SMOTE` object."]},{"cell_type":"markdown","metadata":{"id":"OjNqWrYD34yw"},"source":["If you are getting the right output, great!  Let's compare that to our baseline log loss:"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3D0Ae0d734yx","executionInfo":{"status":"ok","timestamp":1642604912654,"user_tz":300,"elapsed":126,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"7afea384-c1d9-4c28-e000-ba318b5ea4ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.17215925365425272\n","0.13235899551356448\n"]}],"source":["# Run this cell without changes\n","print(-baseline_neg_log_loss_cv.mean())\n","print(-preprocessed_neg_log_loss_cv.mean())"]},{"cell_type":"markdown","metadata":{"id":"J5ASiNZQ34yx"},"source":["Looks like our preprocessing with `StandardScaler` and `SMOTE` has provided some improvement over the baseline! Let's move on to Step 4."]},{"cell_type":"markdown","metadata":{"id":"AtWDn9YA34yx"},"source":["## 4. Build and Evaluate Additional Logistic Regression Models\n","\n","Now that we have applied appropriate preprocessing steps to our data in our custom cross validation function, we can reuse that function to test multiple different `LogisticRegression` models.\n","\n","For each model iteration, make sure you specify `class_weight={1: 0.28}`, because this aligns with the weighting created by our `SMOTE` process.\n","\n","### Where to Next?\n","\n","One of the first questions to ask when you start iterating on any model is: ***are we overfitting***? Many of the models you will learn during this course will have built-in functionality to reduce overfitting.\n","\n","To determine whether we are overfitting, let's examine the training scores vs. the validation scores from our existing modeling process:"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QbvQ4Puo34yx","executionInfo":{"status":"ok","timestamp":1642604917517,"user_tz":300,"elapsed":132,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"fb47b0e5-f2b7-43e1-fe84-e494d85afeaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train:      [0.29227141 0.28801243 0.29282026 0.28652204 0.28910185]\n","Validation: [0.13067576 0.13636961 0.12628191 0.13715658 0.13131112]\n"]}],"source":["# Run this cell without changes\n","print(\"Train:     \", -preprocessed_train_scores)\n","print(\"Validation:\", -preprocessed_neg_log_loss_cv)"]},{"cell_type":"markdown","metadata":{"id":"8MvHeZHb34yx"},"source":["Remember that these are loss metrics, meaning lower is better. Are we overfitting?"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"GI_AIrZk34yy","executionInfo":{"status":"ok","timestamp":1642604992825,"user_tz":300,"elapsed":110,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"f9db3c3c-edcb-4759-b539-8266c5190c60"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nWhile lower is better, if we were overfitting then our scores on training data\\nwould be much higher than the validation data. Because our scores on the\\nvalidation data are better, it's likely we are not overfitting.\\n\""]},"metadata":{},"execution_count":20}],"source":["# Replace None with appropriate text\n","\"\"\"\n","While lower is better, if we were overfitting then our scores on training data\n","would be much higher than the validation data. Because our scores on the\n","validation data are better, it's likely we are not overfitting.\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"u_H37a3F34yy"},"source":["It's actually possible that we are underfitting due to too high of regularization. Remember that `LogisticRegression` from scikit-learn has regularization by default"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WfLFAXA534yy","executionInfo":{"status":"ok","timestamp":1642604995728,"user_tz":300,"elapsed":117,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"44e3a23d-1abe-44a3-c17a-1828122a23b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'C': 1.0,\n"," 'class_weight': {1: 0.28},\n"," 'dual': False,\n"," 'fit_intercept': True,\n"," 'intercept_scaling': 1,\n"," 'l1_ratio': None,\n"," 'max_iter': 100,\n"," 'multi_class': 'auto',\n"," 'n_jobs': None,\n"," 'penalty': 'l2',\n"," 'random_state': 42,\n"," 'solver': 'lbfgs',\n"," 'tol': 0.0001,\n"," 'verbose': 0,\n"," 'warm_start': False}"]},"metadata":{},"execution_count":21}],"source":["# Run this cell without changes\n","model_with_preprocessing.get_params()"]},{"cell_type":"markdown","metadata":{"id":"Rx9bua4A34yy"},"source":["That first key-value pair, `'C': 1.0`, specifies the regularization strength. As is noted in the [scikit-learn `LogisticRegression` docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), `C` is:\n","\n","> Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n","\n","In general if you are increasing `C` you want to increase it by orders of magnitude. I.e. not increasing it to 1.1, but rather increasing it to 1e3, 1e5, etc."]},{"cell_type":"markdown","metadata":{"id":"k_g3Dh5234yy"},"source":["### Reducing Regularization\n","\n","In the cell below, instantiate a `LogisticRegression` model with lower regularization (i.e. higher `C`), along with `random_state=42` and `class_weight={1: 0.28}`. Call this model `model_less_regularization`."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"41ubxeH534yz","executionInfo":{"status":"ok","timestamp":1642605051165,"user_tz":300,"elapsed":108,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}}},"outputs":[],"source":["# Replace None with appropriate code\n","\n","model_less_regularization = LogisticRegression(\n","    random_state = 42,\n","    class_weight={1: 0.28},\n","    C=1e5\n",")"]},{"cell_type":"markdown","metadata":{"id":"BnvY-ZOc34yz"},"source":["This code cell double-checks that the model was created correctly:"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"9nzLKhsM34yz","executionInfo":{"status":"ok","timestamp":1642605054073,"user_tz":300,"elapsed":133,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}}},"outputs":[],"source":["# Run this cell without changes\n","\n","# Check variable type\n","assert type(model_less_regularization) == LogisticRegression\n","\n","# Check params\n","assert model_less_regularization.get_params()[\"random_state\"] == 42\n","assert model_less_regularization.get_params()[\"class_weight\"] == {1: 0.28}\n","assert model_less_regularization.get_params()[\"C\"] != 1.0"]},{"cell_type":"markdown","metadata":{"id":"tLsKlhrd34yz"},"source":["Now, evaluate that model using `custom_cross_val_score`. Recall that `custom_cross_val_score` takes 3 arguments: `estimator`, `X`, and `y`. In this case, `estimator` should be `model_less_regularization`, `X` should be `X_train`, and `y` should be `y_train`."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9akiHD234y0","executionInfo":{"status":"ok","timestamp":1642605095085,"user_tz":300,"elapsed":3519,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"b8149216-6528-4ee1-c1a2-1b63ab3a2b2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous Model\n","Train average:      0.2897456010522682\n","Validation average: 0.13235899551356448\n","Current Model\n","Train average:      0.28957525587270083\n","Validation average: 0.1323443569205194\n"]}],"source":["# Replace None with appropriate code\n","less_regularization_train_scores, less_regularization_val_scores = custom_cross_val_score(\n","    model_less_regularization, X_train, y_train)\n","\n","print(\"Previous Model\")\n","print(\"Train average:     \", -preprocessed_train_scores.mean())\n","print(\"Validation average:\", -preprocessed_neg_log_loss_cv.mean())\n","print(\"Current Model\")\n","print(\"Train average:     \", -less_regularization_train_scores.mean())\n","print(\"Validation average:\", -less_regularization_val_scores.mean())"]},{"cell_type":"markdown","metadata":{"id":"rs3tIeHn34y0"},"source":["Your answers will vary somewhat depending on the value of `C` that you chose, but in general you should see a slight improvement, from something like 0.132358 validation average to 0.132344 (improvement of .000014). Not a massive difference but it is an improvement!"]},{"cell_type":"markdown","metadata":{"id":"606VHI4234y0"},"source":["### Alternative Solver\n","\n","Right now we are using the default solver and type of regularization penalty:"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1WpWqc934y0","executionInfo":{"status":"ok","timestamp":1642605110628,"user_tz":300,"elapsed":115,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"0966e2b4-9f16-4dbd-88bf-e1857b8cee8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["solver: lbfgs\n","penalty: l2\n"]}],"source":["# Run this cell without changes\n","print(\"solver:\", model_less_regularization.get_params()[\"solver\"])\n","print(\"penalty:\", model_less_regularization.get_params()[\"penalty\"])"]},{"cell_type":"markdown","metadata":{"id":"wqchmlLV34y1"},"source":["What if we want to try a different kind of regularization penalty?\n","\n","From the docs:\n","\n","> * ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty\n","> * ‘liblinear’ and ‘saga’ also handle L1 penalty\n","> * ‘saga’ also supports ‘elasticnet’ penalty\n","\n","In other words, the only models that support L1 or elastic net penalties are `liblinear` and `saga`. `liblinear` is going to be quite slow with the size of our dataset, so let's use `saga`.\n","\n","In the cell below, create a model that uses `solver=\"saga\"` and `penalty=\"elasticnet\"`. Then use the `l1_ratio` argument to specify the mixing of L1 and L2 regularization. You want a value greater than zero (zero would just mean using L2 regularization) and less than one (one would mean using just L1 regularization).\n","\n","Remember to also specify `random_state=42`, `class_weight={1: 0.28}`, and `C` equals the value you previously used."]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zzEb7qyM34y1","executionInfo":{"status":"ok","timestamp":1642605217761,"user_tz":300,"elapsed":19140,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"6d3df243-fffc-467b-a3db-b736b4e1f5d5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Previous Model (Less Regularization)\n","Train average:      0.28957525587270083\n","Validation average: 0.1323443569205194\n","Current Model\n","Train average:      0.2929768409986049\n","Validation average: 0.13604493761082836\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n"]}],"source":["# Replace None with appropriate code\n","model_alternative_solver = LogisticRegression(\n","    random_state=42, class_weight = {1: 0.28}, C=1e5, solver='saga',\n","    penalty = 'elasticnet', l1_ratio = 0.5\n",")\n","\n","alternative_solver_train_scores, alternative_solver_val_scores = custom_cross_val_score(\n","    model_alternative_solver,\n","    X_train,\n","    y_train\n",")\n","\n","print(\"Previous Model (Less Regularization)\")\n","print(\"Train average:     \", -less_regularization_train_scores.mean())\n","print(\"Validation average:\", -less_regularization_val_scores.mean())\n","print(\"Current Model\")\n","print(\"Train average:     \", -alternative_solver_train_scores.mean())\n","print(\"Validation average:\", -alternative_solver_val_scores.mean())"]},{"cell_type":"markdown","metadata":{"id":"QWz3Jcwm34y2"},"source":["Most likely you started getting `ConvergenceWarning`s again, even though we are scaling the data inside of `custom_cross_val_score`. When you get a convergence warning in a case like this, you want to modify the `tol` and/or `max_iter` parameters."]},{"cell_type":"markdown","metadata":{"id":"HYZreGah34y2"},"source":["### Adjusting Gradient Descent Parameters\n","\n","If you are getting good results (good metrics) but are still getting a `ConvergenceWarning`, consider increasing the tolerance (`tol` argument). The tolerance specifies how close to zero the gradient must be in order to stop taking additional steps. It's possible that your model is finding a gradient that is close enough to zero, but slightly above the default tolerance, if everything otherwise looks good.\n","\n","In this case, we are getting slightly worse metrics on both the train and the validation data (compared to a different solver strategy), so increasing the number of iterations (`max_iter`) seems like a better strategy. Essentially this is allowing the gradient descent algorithm to take more steps as it tries to find an optimal solution.\n","\n","In the cell below, create a model called `model_more_iterations` that has the same hyperparameters as `model_alternative_solver`, with the addition of an increased `max_iter`. You'll need to increase `max_iter` significantly to a number in the thousands.\n","\n","**Note:** As you increase `max_iter`, it is normal for the execution time of fitting the model to increase. The following cell may take up to several minutes to execute. Try to be patient with this exercise! If this step times out, you can just read on ahead."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kKYDBE-34y2","executionInfo":{"status":"ok","timestamp":1642605546423,"user_tz":300,"elapsed":267254,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"5efeac51-d809-4d6d-f355-4c090ba97adc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Previous Best Model (Less Regularization)\n","Train average:      0.28957525587270083\n","Validation average: 0.1323443569205194\n","Previous Model with This Solver\n","Train average:      0.2929768409986049\n","Validation average: 0.13604493761082836\n","Current Model\n","Train average:      0.2897322023187443\n","Validation average: 0.13241907028228447\n"]}],"source":["# Replace None with appropriate code\n","model_more_iterations = LogisticRegression(\n","    random_state = 42, class_weight = {1: 0.28}, C=1e5, solver='saga',\n","    penalty = 'elasticnet', l1_ratio = 0.5, max_iter = 2000\n",")\n","\n","more_iterations_train_scores, more_iterations_val_scores = custom_cross_val_score(\n","    model_more_iterations,\n","    X_train,\n","    y_train\n",")\n","\n","print(\"Previous Best Model (Less Regularization)\")\n","print(\"Train average:     \", -less_regularization_train_scores.mean())\n","print(\"Validation average:\", -less_regularization_val_scores.mean())\n","print(\"Previous Model with This Solver\")\n","print(\"Train average:     \", -alternative_solver_train_scores.mean())\n","print(\"Validation average:\", -alternative_solver_val_scores.mean())\n","print(\"Current Model\")\n","print(\"Train average:     \", -more_iterations_train_scores.mean())\n","print(\"Validation average:\", -more_iterations_val_scores.mean())"]},{"cell_type":"markdown","metadata":{"id":"XXf0flsQ34y2"},"source":["The results you got are most likely around 0.13241, whereas the previous model was around 0.13234. In other words, even after waiting all that time, we are getting 0.00007 worse log loss with this solver.\n","\n","This is a fairly typical experience when hyperparameter tuning! Often the default hyperparameters are the default because they work best in the most situations. This is especially true of logistic regression, which has relatively few hyperparameters. Once we get to more complex models, there are more \"levers to pull\" (hyperparameters to adjust) so it is more likely that we'll improve performance by deviating from the default.\n","\n","Let's declare the `model_less_regularization` to be our best model, and move on to the final evaluation phase."]},{"cell_type":"markdown","metadata":{"id":"iUIVF8NS34y2"},"source":["## 5. Choose and Evaluate a Final Model"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"yaji1__r34y3","executionInfo":{"status":"ok","timestamp":1642605599733,"user_tz":300,"elapsed":165,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}}},"outputs":[],"source":["# Run this cell without changes\n","final_model = model_less_regularization"]},{"cell_type":"markdown","metadata":{"id":"EUjIwgnm34y3"},"source":["In order to evaluate our final model, we need to preprocess the full training and test data, fit the model on the full training data, and evaluate it on the full test data. Initially we'll continue to use log loss for the evaluation step.\n","\n","### Preprocessing the Full Dataset"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"ww_l20nz34y3","executionInfo":{"status":"ok","timestamp":1642605708995,"user_tz":300,"elapsed":388,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}}},"outputs":[],"source":["# Replace None with appropriate code\n","\n","# Instantiate StandardScaler\n","scaler = StandardScaler()\n","# Fit and transform X_train\n","X_train_scaled = scaler.fit_transform(X_train)\n","# Transform X_test\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Instantiate SMOTE with random_state=42 and sampling_strategy=0.28\n","sm = SMOTE(random_state=42, sampling_strategy=0.28)\n","# Fit and transform X_train_scaled and y_train using sm\n","X_train_oversampled, y_train_oversampled = sm.fit_resample(X_train_scaled, y_train)"]},{"cell_type":"markdown","metadata":{"id":"4zXUW9qF34y3"},"source":["### Fitting the Model on the Full Training Data"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S2ZlX6WV34y3","executionInfo":{"status":"ok","timestamp":1642605716364,"user_tz":300,"elapsed":875,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"f42067d9-440c-4987-d120-70b80e1f35c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=100000.0, class_weight={1: 0.28}, random_state=42)"]},"metadata":{},"execution_count":32}],"source":["# Run this cell without changes\n","final_model.fit(X_train_oversampled, y_train_oversampled)"]},{"cell_type":"markdown","metadata":{"id":"v6WsM__634y3"},"source":["### Evaluating the Model on the Test Data\n","\n","#### Log Loss"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NlzyzaUR34y3","executionInfo":{"status":"ok","timestamp":1642605720881,"user_tz":300,"elapsed":106,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"64851b59-46d1-4f18-d8cf-5ef99615d0f9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.13031294393914256"]},"metadata":{},"execution_count":33}],"source":["# Run this cell without changes\n","log_loss(y_test, final_model.predict_proba(X_test_scaled))"]},{"cell_type":"markdown","metadata":{"id":"lhVrLI2334y4"},"source":["Great! We are getting slightly better performance when we train the model with the full training set, compared to the average cross-validated performance. This is typical since models tend to perform better with more training data.\n","\n","This model has improved log loss compared to our initial baseline model, which had about 0.172.\n","\n","But we're not quite done here!\n","\n","If we wanted to present this forest cover classification model to non-technical stakeholders, log loss would be a confusing choice. Let's compute some other metrics that tell the story of our model's performance in a more interpretable way."]},{"cell_type":"markdown","metadata":{"id":"1TepSWrt34y4"},"source":["#### Accuracy\n","\n","Although we noted the issues with accuracy as a metric on unbalanced datasets, accuracy is a very intuitive metric. Recall that we would expect an accuracy of about 0.928651 if we identified every cell as class 0. What accuracy do we get with our new model?\n","\n","(Note that we used `.predict_proba` above, but accuracy uses `.predict`)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaA1nQek34y4","executionInfo":{"status":"ok","timestamp":1642605752344,"user_tz":300,"elapsed":142,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"a5b1eab9-2808-489b-c480-729ee7e95b66"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9456679825472678"]},"metadata":{},"execution_count":34}],"source":["# Replace None with appropriate code\n","\n","from sklearn.metrics import accuracy_score\n","\n","accuracy_score(y_test, final_model.predict(X_test_scaled))"]},{"cell_type":"markdown","metadata":{"id":"Qidtp5Qc34y4"},"source":["In other words, our model correctly identifies the type of forest cover about 94.6% of the time, whereas always guessing the majority class (ponderosa pine) would only be accurate about 92.9% of the time."]},{"cell_type":"markdown","metadata":{"id":"f15AKBqS34y4"},"source":["#### Precision\n","\n","If we always chose the majority class, we would expect a precision of 0, since we would never identify any \"true positives\". What is the precision of our final model? Use `precision_score` from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html))."]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zAaOxR3e34y4","executionInfo":{"status":"ok","timestamp":1642605794699,"user_tz":300,"elapsed":112,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"21e4b5ab-b1e2-4769-a052-b8d6a7949c25"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6659919028340081"]},"metadata":{},"execution_count":35}],"source":["# Replace None with appropriate code\n","\n","# Import the relevant function\n","from sklearn.metrics import precision_score\n","\n","# Display the precision score\n","precision_score(y_test, final_model.predict(X_test_scaled))"]},{"cell_type":"markdown","metadata":{"id":"E1TnzUjT34y4"},"source":["In other words, if our model labels a given cell of forest as class 1, there is about a 66.6% chance that it is actually class 1 (cottonwood/willow) and about a 33.4% chance that it is actually class 0 (ponderosa pine)."]},{"cell_type":"markdown","metadata":{"id":"GgrFDedX34y5"},"source":["#### Recall\n","\n","Again, if we always chose the majority class, we would expect a recall of 0. What is the recall of our final model? Use `recall_score` from scikit-learn ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html))."]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aw9gKW3i34y5","executionInfo":{"status":"ok","timestamp":1642605832814,"user_tz":300,"elapsed":123,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"362b38b7-da3a-4594-9f54-01e51753f650"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.47889374090247455"]},"metadata":{},"execution_count":36}],"source":["# Replace None with appropriate code\n","\n","# Import the relevant function\n","from sklearn.metrics import recall_score\n","\n","# Display the recall score\n","recall_score(y_test, final_model.predict(X_test_scaled))"]},{"cell_type":"markdown","metadata":{"id":"UipiuND934y5"},"source":["In other words, if a given cell of forest is actually class 1, there is about a 47.9% chance that our model will correctly label it as class 1 (cottonwood/willow) and about a 52.1% chance that our model will incorrectly label it as class 0 (ponderosa pine)."]},{"cell_type":"markdown","metadata":{"id":"nr1X42X934y5"},"source":["#### Interpretation\n","\n","Depending on the stakeholder, you most likely want to report just precision or just recall. Try to understand their business case:\n","\n","* If false positives are a bigger problem (labeled cottonwood/willow when it's really ponderosa pine), precision is the important metric to report\n","* If false negatives are a bigger problem (labeled ponderosa pine when it's really cottonwood/willow), recall is the important metric to report\n","\n","If those problems have truly equal importance, you could report an f1-score instead, although this is somewhat more difficult for the average person to interpret.\n","\n","#### BONUS: Adjusting the Decision Threshold\n","\n","If either of those problems is important enough that it outweighs overall accuracy, you could also adjust the decision threshold of your final model to improve the metric that matters most. Let's say that it's important to improve the recall score — that we want to be able to correctly label more of the cottonwood/willow trees as cottonwood/willow trees, even if that means accidentally labeling more ponderosa pine as cottonwood/willow incorrectly.\n","\n","Then we can use `.predict_proba` to err on the side of the positive class. Let's use an exaggerated example, which assumes that false negatives are a very significant problem. Instead of using the default 50% threshold (where a probability over 0.5 is classified as positive) let's say that if there is greater than a 1% chance it's positive, we classify it as positive:\n","\n","(If the opposite issue were the case — it's very important that every area classified as 1 is actually cottonwood/willow — you would want the threshold to be higher than 50% rather than lower than 50%.)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ltxK-pby34y5","executionInfo":{"status":"ok","timestamp":1642605846176,"user_tz":300,"elapsed":115,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"a5150075-f816-4744-b879-a6f73930fe90"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    0.586433\n","1    0.413567\n","dtype: float64"]},"metadata":{},"execution_count":37}],"source":["# Run this cell without changes\n","\n","def final_model_func(model, X):\n","    \"\"\"\n","    Custom function to predict probability of\n","    cottonwood/willow. If the model says there\n","    is >1% chance, we label it as class 1\n","    \"\"\"\n","    probs = model.predict_proba(X)[:,1]\n","    return [int(prob > 0.01) for prob in probs]\n","\n","# Calculate predictions with adjusted threshold and\n","# display proportions of predictions\n","threshold_adjusted_probs = pd.Series(final_model_func(final_model, X_test_scaled))\n","threshold_adjusted_probs.value_counts(normalize=True)"]},{"cell_type":"markdown","metadata":{"id":"FrseNSo634y5"},"source":["So, now we are predicting that everything over a 1% chance of being class 1 as class 1, which means that we're classifying about 58.6% of the records as class 0 and 41.4% of the records as class 1."]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ngGM_Vdb34y6","executionInfo":{"status":"ok","timestamp":1642605850466,"user_tz":300,"elapsed":144,"user":{"displayName":"Brian Tracy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh3fdZyeAX0uksWlqOlHe2C7sPM3bDBKezi9ei5=s64","userId":"16968356164364792516"}},"outputId":"5dbddd68-b882-4fc9-fc7e-eaf652a7ba6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.6565551630999377\n","Recall:   0.9912663755458515\n"]}],"source":["# Run this cell without changes\n","print(\"Accuracy:\", accuracy_score(y_test, threshold_adjusted_probs))\n","print(\"Recall:  \", recall_score(y_test, threshold_adjusted_probs))"]},{"cell_type":"markdown","metadata":{"id":"fv26NReL34y6"},"source":["This means that we are able to identify 99.1% of the true positives (i.e. 99.1% of the cottonwood/willow cells are identified). However this comes at a cost; our overall accuracy is now 65.7% instead of over 90%.\n","\n","So we are classifying over 40% of the cells as cottonwood/willow, even though fewer than 10% of the cells are actually that category, in order to miss as few true positives as possible. Even though this seems fairly extreme, our model is still better than just choosing class 1 every time (that model would have about 7% accuracy).\n","\n","This kind of model might be useful if there is some kind of treatment needed for cottonwood/willow trees, but your organization only has the resources to visit fewer than half of the study areas. This model would allow them to visit 41% of the areas and successfully treat over 99% of the cottonwood/willow trees.\n","\n","You can also imagine a less-drastic version of this threshold adjusting, where you trade off a marginal improvement in precision or recall for a marginal reduction in accuracy. Visually inspecting the precision-recall curve ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_precision_recall_curve.html)) can help you choose the threshold based on what you want to optimize."]},{"cell_type":"markdown","metadata":{"id":"znCAt_ix34y6"},"source":["## Summary\n","\n","In this lab, you completed an end-to-end machine learning modeling process with logistic regression on an imbalanced dataset. First you built and evaluated a baseline model. Next you wrote a custom cross validation function in order to use SMOTE resampling appropriately (without needing an `imblearn` pipeline). After that, you tuned the model through adjusting the regularization strength and the gradient descent hyperparameters. Finally, you evaluated your final model on log loss as well as more user-friendly metrics."]}],"metadata":{"kernelspec":{"display_name":"Python (learn-env)","language":"python","name":"learn-env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"index.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}